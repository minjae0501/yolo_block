{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minjae0501/yolo_block/blob/master/lstm_yolo_detecting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cECJ3kW1kWsW"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCnOheVr-qBn",
        "outputId": "553d50a6-2eff-47a4-b849-d1a3a64986e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'yolo_block'...\n",
            "remote: Enumerating objects: 3012, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 3012 (delta 3), reused 13 (delta 1), pack-reused 2993\u001b[K\n",
            "Receiving objects: 100% (3012/3012), 117.88 MiB | 50.70 MiB/s, done.\n",
            "Resolving deltas: 100% (50/50), done.\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/minjae0501/yolo_block.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6AKCg3M-pCz",
        "outputId": "919a6b1f-e3a5-4469-bc07-57a8d523aa5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.227 üöÄ Python-3.9.13 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce RTX 2070, 8192MiB)\n",
            "Setup complete ‚úÖ (8 CPUs, 15.9 GB RAM, 227.4/232.3 GB disk)\n"
          ]
        }
      ],
      "source": [
        "# %pip install ultralytics\n",
        "# %pip install mediapipe\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Na8wp-xB-pC2"
      },
      "outputs": [],
      "source": [
        "\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "from ultralytics import YOLO\n",
        "import numpy as np\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIxePsio-pC3",
        "outputId": "62894ebe-f1c2-41d5-f528-f1c98b44431c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available() == True:\n",
        "    device = 'cuda:0'\n",
        "else:\n",
        "    device = 'cpu'\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, seq_list):\n",
        "        self.X = []\n",
        "        self.y = []\n",
        "        for dic in seq_list:\n",
        "            self.y.append(dic['key'])\n",
        "            self.X.append(dic['value'])\n",
        "    def __getitem__(self, index):\n",
        "        data = self.X[index]\n",
        "        label = self.y[index]\n",
        "        return torch.Tensor(np.array(data)), torch.tensor(np.array(int(label)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lX-RhFX5-pC3"
      },
      "outputs": [],
      "source": [
        "class hand_LSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(hand_LSTM, self).__init__()\n",
        "        self.lstm1 = nn.LSTM(input_size = 67, hidden_size = 128, num_layers=1, batch_first=True) # xyz 63 + x_center, y_center = 65\n",
        "        # self.lstm2 = nn.LSTM(input_size = 128, hidden_size = 256, num_layers=1, batch_first=True)\n",
        "        # self.lstm3 = nn.LSTM(input_size = 256, hidden_size = 512, num_layers=1, batch_first=True)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        # self.lstm4 = nn.LSTM(input_size = 512, hidden_size = 256, num_layers=1, batch_first=True)\n",
        "        # self.lstm5 = nn.LSTM(input_size = 256, hidden_size = 128, num_layers=1, batch_first=True)\n",
        "        self.lstm6 = nn.LSTM(input_size = 128, hidden_size = 64, num_layers=1, batch_first=True)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "        self.lstm7 = nn.LSTM(input_size = 64, hidden_size = 32, num_layers=1, batch_first=True)\n",
        "        self.fc = nn.Linear(32, 2)\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm1(x)\n",
        "        # x, _ = self.lstm2(x)\n",
        "        # x, _ = self.lstm3(x)\n",
        "        x = self.dropout1(x)\n",
        "        # x, _ = self.lstm4(x)\n",
        "        # x, _ = self.lstm5(x)\n",
        "        x, _ = self.lstm6(x)\n",
        "        x = self.dropout2(x)\n",
        "        x, _ = self.lstm7(x)\n",
        "        x = self.fc(x[:, -1,:])\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def grab_release(image, yolo_model, hand_list , lstm_model, detect_cls, hand_cls, length, xyz_list_list, status, status_num):\n",
        "    mp_hands, hands, mp_drawing = hand_list[0], hand_list[1], hand_list[2]\n",
        "\n",
        "    # YOLO Í∞ùÏ≤¥ Í∞êÏßÄ\n",
        "    box_results = yolo_model.predict(image, conf = 0.6, verbose=False, show = False)\n",
        "    boxes = box_results[0].boxes.xyxy.cpu()\n",
        "    box_class = box_results[0].boxes.cls.cpu().tolist()\n",
        "\n",
        "    x1, y1, x2, y2 = 0, 0, 0, 0\n",
        "    hx1, hy1, hx2, hy2 = 0,0,0,0\n",
        "    for idx, cls in enumerate(box_class):\n",
        "        if int(cls) == detect_cls:\n",
        "            x1, y1, x2, y2 = boxes[idx]\n",
        "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
        "            cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "        elif int(cls) == hand_cls:\n",
        "            hx1, hy1, hx2, hy2 = boxes[idx]\n",
        "            hx1, hy1, hx2, hy2 = int(hx1), int(hy1), int(hx2), int(hy2)\n",
        "            cv2.rectangle(image, (int(hx1), int(hy1)), (int(hx2), int(hy2)), (0, 0, 255), 2)\n",
        "    \n",
        "    #mediapipe\n",
        "    results = hands.process(image)\n",
        "    xyz_list = []\n",
        "    if results.multi_hand_landmarks:\n",
        "        for x_y_z in results.multi_hand_landmarks:\n",
        "            for landmark in x_y_z.landmark:\n",
        "                xyz_list.append(landmark.x)\n",
        "                xyz_list.append(landmark.y)\n",
        "                xyz_list.append(landmark.z)\n",
        "\n",
        "        xyz_list.append(abs(x1-hx1)/640)\n",
        "        xyz_list.append(abs(x2-hx2)/640)\n",
        "        xyz_list.append(abs(y1-hy1)/640)\n",
        "        xyz_list.append(abs(y2-hy2)/640)\n",
        "\n",
        "        if x1 != 0 and y1 != 0 and x2 != 0 and y2 != 0 and hx1 != 0 and hy1 != 0 and hx2 != 0 and hy2 != 0:\n",
        "            xyz_list_list.append(xyz_list)\n",
        "\n",
        "        for hand_landmarks in results.multi_hand_landmarks:\n",
        "              with torch.no_grad():\n",
        "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "    \n",
        "    if len(xyz_list_list) == length:\n",
        "        dataset = []\n",
        "        dataset.append({'key': 0, 'value': xyz_list_list})\n",
        "        dataset = MyDataset(dataset)\n",
        "        dataset = DataLoader(dataset)\n",
        "        xyz_list_list = []\n",
        "        for data, label in dataset:\n",
        "            data = data.to(device)\n",
        "            with torch.no_grad():\n",
        "                result = lstm_model(data)\n",
        "                _, out = torch.max(result, 1)\n",
        "                status_num = out.item()\n",
        "                if out.item() == 0: status = 'Release'\n",
        "                else: status = 'Grab'\n",
        "\n",
        "    # cv2.putText(image, status, (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1.5, (0,0, 225), 2)\n",
        "\n",
        "    return image, xyz_list_list, status, status_num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yaefZpU-pC4",
        "outputId": "4c04ad5d-8c10-4c5d-df5b-b09a6bc7ed68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Î™®Îç∏Ïù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Î∂àÎü¨ÏôÄÏ°åÏäµÎãàÎã§.\n"
          ]
        }
      ],
      "source": [
        "# lstmÎ™®Îç∏ Î∂àÎü¨Ïò§Í∏∞\n",
        "model_path = './lstm_pth/lstm_model.pth'\n",
        "\n",
        "lstm_model = hand_LSTM().to(device)\n",
        "lstm_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "lstm_model.eval()\n",
        "print(\"Î™®Îç∏Ïù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Î∂àÎü¨ÏôÄÏ°åÏäµÎãàÎã§.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# mediapipe ÏÜê Í∞êÏßÄ Î™®Îìà Ï¥àÍ∏∞Ìôî\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.3)\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "hand_list = [mp_hands, hands, mp_drawing]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOLO Í∞ùÏ≤¥ Í∞êÏßÄ Î™®Îç∏ Ï¥àÍ∏∞Ìôî\n",
        "best_model = 'block_best_02.pt'\n",
        "yolo_model = YOLO(best_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOLO Ïä§ÌÉ≠ ÌÉêÏßÄ Î™®Îç∏\n",
        "step_best_model = 'truck_best_03.pt'\n",
        "step_model = YOLO(step_best_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ÏÜê, datasetÍ∏∏Ïù¥ ÏÑ§Ï†ï\n",
        "hand_cls = 6\n",
        "length = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "FH2UhLBz-pC7"
      },
      "outputs": [],
      "source": [
        "cap_device = 0\n",
        "cap = cv2.VideoCapture(cap_device) # for Mac\n",
        "# cap = cv2.VideoCapture(0) # for Windows\n",
        "\n",
        "lstm_model.eval()\n",
        "status = 'None'\n",
        "status_num = -1\n",
        "step = 0\n",
        "wait_frames = 60\n",
        "cnt_frames = 0\n",
        "xyz_list_list = []\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    \n",
        "    image = cv2.resize(frame, (640, 640))\n",
        "\n",
        "    box_results = step_model.predict(image, conf = 0.6, verbose=False, show = False)\n",
        "    boxes = box_results[0].boxes.xyxy.cpu()\n",
        "    box_class = box_results[0].boxes.cls.cpu().tolist()\n",
        "\n",
        "    x1, y1, x2, y2 = 0, 0, 0, 0\n",
        "    for idx, cls in enumerate(box_class):\n",
        "        if int(cls) == step:\n",
        "            x1, y1, x2, y2 = boxes[idx]\n",
        "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
        "            cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (225, 0, 0), 2)\n",
        "    \n",
        "    if step == 0 :\n",
        "        detect_cls = 0\n",
        "\n",
        "        image, xyz_list_list, status, status_num = grab_release(image, yolo_model, hand_list , lstm_model, detect_cls, hand_cls, length, xyz_list_list, status, status_num)\n",
        "\n",
        "        if status_num == 1:\n",
        "            text = 'Put the blue bridge over the red circle'\n",
        "        else:\n",
        "            text = 'Pick up the blue bridge'\n",
        "        \n",
        "\n",
        "        if x1 != 0 and y1 != 0 and x2 != 0 and y2 != 0:\n",
        "            if cnt_frames != wait_frames:\n",
        "                text = 'Good Job!!'\n",
        "                cnt_frames += 1\n",
        "            else:\n",
        "                step += 1\n",
        "                cnt_frames = 0\n",
        "                status = 'None'\n",
        "                status_num = -1\n",
        "        cv2.putText(image, text, (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0, 225), 1)\n",
        "\n",
        "    elif step == 1:\n",
        "        detect_cls = 13\n",
        "\n",
        "        image, xyz_list_list, status, status_num = grab_release(image, yolo_model, hand_list , lstm_model, detect_cls, hand_cls, length, xyz_list_list, status, status_num)\n",
        "\n",
        "        if status_num == 1:\n",
        "            text = 'Put the yellow bridge on the left side of the blue bridge'\n",
        "        else:\n",
        "            text = 'Pick up the yellow bridge'\n",
        "\n",
        "        if x1 != 0 and y1 != 0 and x2 != 0 and y2 != 0:\n",
        "            if cnt_frames != wait_frames:\n",
        "                text = 'Good Job!!'\n",
        "                cnt_frames += 1\n",
        "            else:\n",
        "                step += 1\n",
        "                cnt_frames = 0\n",
        "                status = 'None'\n",
        "                status_num = -1\n",
        "\n",
        "        cv2.putText(image, text, (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0, 225), 1)\n",
        "    \n",
        "    elif step == 2:\n",
        "        detect_cls = 4\n",
        "\n",
        "        image, xyz_list_list, status, status_num = grab_release(image, yolo_model, hand_list , lstm_model, detect_cls, hand_cls, length, xyz_list_list, status, status_num)\n",
        "\n",
        "        if status_num == 1:\n",
        "            text = 'Put the green circle under the yellow bridge'\n",
        "        else:\n",
        "            text = 'Pick up the green circle'\n",
        "\n",
        "        if x1 != 0 and y1 != 0 and x2 != 0 and y2 != 0:\n",
        "            if cnt_frames != wait_frames:\n",
        "                text = 'Good Job!!'\n",
        "                cnt_frames += 1\n",
        "            else:\n",
        "                step += 1\n",
        "                cnt_frames = 0\n",
        "                status = 'None'\n",
        "                status_num = -1\n",
        "                \n",
        "        cv2.putText(image, text, (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0, 225), 1)\n",
        "    \n",
        "    elif step == 3:\n",
        "        detect_cls = 5\n",
        "\n",
        "        image, xyz_list_list, status, status_num = grab_release(image, yolo_model, hand_list , lstm_model, detect_cls, hand_cls, length, xyz_list_list, status, status_num)\n",
        "\n",
        "        if status_num == 1:\n",
        "            text = 'Put the green cube on the top left of the yellow bridge'\n",
        "        else:\n",
        "            text = 'Pick up the green cube'\n",
        "\n",
        "        if x1 != 0 and y1 != 0 and x2 != 0 and y2 != 0:\n",
        "            if cnt_frames != wait_frames:\n",
        "                text = 'Good Job!!'\n",
        "                cnt_frames += 1\n",
        "            else:\n",
        "                step += 1\n",
        "                cnt_frames = 0\n",
        "                status = 'None'\n",
        "                status_num = -1\n",
        "                \n",
        "        cv2.putText(image, text, (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0, 225), 1)\n",
        "    \n",
        "    elif step == 4:\n",
        "        detect_cls = 3\n",
        "\n",
        "        image, xyz_list_list, status, status_num = grab_release(image, yolo_model, hand_list , lstm_model, detect_cls, hand_cls, length, xyz_list_list, status, status_num)\n",
        "\n",
        "        if status_num == 1:\n",
        "            text = 'Put the blue quarter of circle on the right side of the green cube'\n",
        "        else:\n",
        "            text = 'Pick up the blue quarter of circle'\n",
        "\n",
        "        if x1 != 0 and y1 != 0 and x2 != 0 and y2 != 0:\n",
        "            if cnt_frames != wait_frames:\n",
        "                text = 'Good Job!!'\n",
        "                # cnt_frames += 1\n",
        "            # else:\n",
        "            #     step += 1\n",
        "            #     cnt_frames = 0\n",
        "            #     status = 'None'\n",
        "            #     status_num = -1\n",
        "                \n",
        "        cv2.putText(image, text, (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0, 225), 1)\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    # image = cv2.resize(image, (1440, 640))\n",
        "    cv2.imshow('frame',image)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
