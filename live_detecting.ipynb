{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minjae0501/yolo_block/blob/master/lstm_yolo_detecting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cECJ3kW1kWsW"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCnOheVr-qBn",
        "outputId": "553d50a6-2eff-47a4-b849-d1a3a64986e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'yolo_block'...\n",
            "remote: Enumerating objects: 3012, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 3012 (delta 3), reused 13 (delta 1), pack-reused 2993\u001b[K\n",
            "Receiving objects: 100% (3012/3012), 117.88 MiB | 50.70 MiB/s, done.\n",
            "Resolving deltas: 100% (50/50), done.\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/minjae0501/yolo_block.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6AKCg3M-pCz",
        "outputId": "919a6b1f-e3a5-4469-bc07-57a8d523aa5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.227 ğŸš€ Python-3.9.13 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce RTX 2070, 8192MiB)\n",
            "Setup complete âœ… (8 CPUs, 15.9 GB RAM, 208.7/232.3 GB disk)\n"
          ]
        }
      ],
      "source": [
        "# %pip install ultralytics\n",
        "# %pip install mediapipe\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Na8wp-xB-pC2"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from ultralytics import YOLO\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import ImageFont, ImageDraw, Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIxePsio-pC3",
        "outputId": "62894ebe-f1c2-41d5-f528-f1c98b44431c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available() == True:\n",
        "    device = 'cuda:0'\n",
        "else:\n",
        "    device = 'cpu'\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, seq_list):\n",
        "        self.X = []\n",
        "        self.y = []\n",
        "        for dic in seq_list:\n",
        "            self.y.append(dic['key'])\n",
        "            self.X.append(dic['value'])\n",
        "    def __getitem__(self, index):\n",
        "        data = self.X[index]\n",
        "        label = self.y[index]\n",
        "        return torch.Tensor(np.array(data)), torch.tensor(np.array(int(label)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ver 9\n",
        "class hand_LSTM(nn.Module):\n",
        "    def __init__(self, num_layers=1):\n",
        "        super(hand_LSTM, self).__init__()\n",
        "        \"\"\"\n",
        "        LayerNorm(): RNNê³¼ LSTMì— ì í•© \n",
        "        - LSTMê³¼ ê°™ì€ ìˆœí™˜ ì‹ ê²½ë§ì—ì„œëŠ” ì‹œê°„ì— ë”°ë¥¸ ì˜ì¡´ì„± ë•Œë¬¸ì— ë°°ì¹˜ ì •ê·œí™”ê°€ ì˜ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.\n",
        "        - ë°˜ë©´ ë ˆì´ì–´ ì •ê·œí™”ëŠ” ì‹œê°„ì  ì˜ì¡´ì„±ì— ì˜í–¥ì„ ë°›ì§€ ì•Šì•„ RNNê³¼ LSTMì— ë” ì í•©í•˜ë‹¤.\n",
        "        \"\"\"\n",
        "        # bidirectional -> ì–‘ë°©í–¥ lstm: ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ìˆœë°©í–¥ê³¼ ì—­ë°©í–¥ ëª¨ë‘ í•™ìŠµ\n",
        "        self.lstm1 = nn.LSTM(67, 128, num_layers, batch_first=True, bidirectional=True)\n",
        "        # lstm layer ì •ê·œí™” ì‚¬ìš©, ì–‘ë°©í–¥ì´ê¸° ë•Œë¬¸ì— 256ê°œ \n",
        "        self.layer_norm1 = nn.LayerNorm(256)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        \n",
        "        self.lstm2 = nn.LSTM(256, 64, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.layer_norm2 = nn.LayerNorm(128)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "        \n",
        "        self.lstm3 = nn.LSTM(128, 32, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.layer_norm3 = nn.LayerNorm(64)\n",
        "        self.dropout3 = nn.Dropout(0.1)\n",
        "        \n",
        "        self.attention = nn.Linear(64, 1)\n",
        "        self.fc = nn.Linear(64, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm1(x)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        \n",
        "        x, _ = self.lstm2(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        \n",
        "        x, _ = self.lstm3(x)\n",
        "        x = self.layer_norm3(x)\n",
        "        x = self.dropout3(x)\n",
        "        \n",
        "        # Attention ë©”ì»¤ë‹ˆì¦˜\n",
        "        attention_weights = torch.softmax(self.attention(x), dim=1)\n",
        "        x = torch.sum(attention_weights * x, dim=1)\n",
        "        \n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def grab_release(image, yolo_model, hand_list , lstm_model, detect_cls, hand_cls, length, xyz_list_list, status_num):\n",
        "    mp_hands, hands, mp_drawing = hand_list[0], hand_list[1], hand_list[2]\n",
        "\n",
        "    # YOLO ê°ì²´ ê°ì§€\n",
        "    box_results = yolo_model.predict(image, conf = 0.6, verbose=False, show = False)\n",
        "    boxes = box_results[0].boxes.xyxy.cpu()\n",
        "    box_class = box_results[0].boxes.cls.cpu().tolist()\n",
        "\n",
        "    x1, y1, x2, y2 = 0, 0, 0, 0\n",
        "    hx1, hy1, hx2, hy2 = 0,0,0,0\n",
        "    for idx, cls in enumerate(box_class):\n",
        "        if int(cls) == detect_cls:\n",
        "            x1, y1, x2, y2 = boxes[idx]\n",
        "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
        "            cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "        elif int(cls) == hand_cls:\n",
        "            hx1, hy1, hx2, hy2 = boxes[idx]\n",
        "            hx1, hy1, hx2, hy2 = int(hx1), int(hy1), int(hx2), int(hy2)\n",
        "            cv2.rectangle(image, (int(hx1), int(hy1)), (int(hx2), int(hy2)), (0, 0, 255), 2)\n",
        "    \n",
        "    #mediapipe\n",
        "    results = hands.process(image)\n",
        "    xyz_list = []\n",
        "    if results.multi_hand_landmarks:\n",
        "        for x_y_z in results.multi_hand_landmarks:\n",
        "            for landmark in x_y_z.landmark:\n",
        "                xyz_list.append(landmark.x) # *10 ì‚­ì œ\n",
        "                xyz_list.append(landmark.y)\n",
        "                xyz_list.append(landmark.z) \n",
        "                \n",
        "\n",
        "        xyz_list.append(abs(x1-hx1)/640) # /640 ì¶”ê°€\n",
        "        xyz_list.append(abs(x2-hx2)/640)\n",
        "        xyz_list.append(abs(y1-hy1)/640)\n",
        "        xyz_list.append(abs(y2-hy2)/640)\n",
        "\n",
        "        if x1 != 0 and y1 != 0 and x2 != 0 and y2 != 0 and hx1 != 0 and hy1 != 0 and hx2 != 0 and hy2 != 0:\n",
        "            xyz_list_list.append(xyz_list)# ê°ì²´ì™€ ì†ì´ ì¸ì‹ë˜ë©´\n",
        "\n",
        "        for hand_landmarks in results.multi_hand_landmarks:\n",
        "              with torch.no_grad():\n",
        "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "    \n",
        "    if len(xyz_list_list) == length:\n",
        "        dataset = []\n",
        "        dataset.append({'key': 0, 'value': xyz_list_list})\n",
        "        dataset = MyDataset(dataset)\n",
        "        dataset = DataLoader(dataset)\n",
        "        xyz_list_list = []\n",
        "        for data, label in dataset:\n",
        "            data = data.to(device)\n",
        "            with torch.no_grad():\n",
        "                result = lstm_model(data)\n",
        "                _, out = torch.max(result, 1)\n",
        "                status_num = out.item()\n",
        "                # if out.item() == 0: status = 'Release'\n",
        "                # else: status = 'Grab'\n",
        "\n",
        "    # cv2.putText(image, status, (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1.5, (0,0, 225), 2)\n",
        "\n",
        "    return image, xyz_list_list, status_num\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í•œê¸€í°íŠ¸ ì¶œë ¥\n",
        "def putText_korean(img, text, position, font_path, font_size, color):\n",
        "    img_pil = Image.fromarray(img)\n",
        "    draw = ImageDraw.Draw(img_pil)\n",
        "    font = ImageFont.truetype(font_path, font_size)\n",
        "    draw.text(position, text, font=font, fill=color)\n",
        "    return np.array(img_pil)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yaefZpU-pC4",
        "outputId": "4c04ad5d-8c10-4c5d-df5b-b09a6bc7ed68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™€ì¡ŒìŠµë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "# YOLO ê°ì²´ ê°ì§€ ëª¨ë¸ ì´ˆê¸°í™”\n",
        "best_model = './block_model/block_best_02.pt'\n",
        "yolo_model = YOLO(best_model)\n",
        "\n",
        "# YOLO ìŠ¤íƒ­ íƒì§€ ëª¨ë¸\n",
        "step_best_model = './block_model/truck_best_04.pt'\n",
        "step_model = YOLO(step_best_model)\n",
        "\n",
        "# lstmëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "model_path = './lstm_pth/more_data_lstm_model_ver9.pth'\n",
        "lstm_model = hand_LSTM().to(device)\n",
        "lstm_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "lstm_model.eval()\n",
        "print(\"ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™€ì¡ŒìŠµë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# mediapipe ì† ê°ì§€ ëª¨ë“ˆ ì´ˆê¸°í™”\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.3)\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "hand_list = [mp_hands, hands, mp_drawing]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì†, datasetê¸¸ì´ ì„¤ì •\n",
        "hand_cls = 6\n",
        "length = 10\n",
        "\n",
        "# í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•  ìœ„ì¹˜, í°íŠ¸ê²½ë¡œ, í°íŠ¸í¬ê¸°, ìƒ‰ìƒì„¤ì •\n",
        "position = (15, 40) # í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥í•  ìœ„ì¹˜\n",
        "font_path = './font/KCC-Ganpan.ttf' # í•œê¸€ í°íŠ¸ íŒŒì¼ ê²½ë¡œ\n",
        "font_size = 30 # í°íŠ¸ í¬ê¸°\n",
        "color = (0, 0, 255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì‹¤ì‹œê°„ ì•ˆë‚´ system í•¨ìˆ˜í™”\n",
        "def detect_and_process(image, step_model, yolo_model, hand_list, lstm_model, step, detect_list, hand_cls, length, xyz_list_list, status_num): # detect_clsì‚­ì œ, step, detect_listì¶”ê°€\n",
        "    box_results = step_model.predict(image, conf = 0.6, verbose=False, show = False)\n",
        "    boxes = box_results[0].boxes.xyxy.cpu()\n",
        "    box_class = box_results[0].boxes.cls.cpu().tolist()\n",
        "    \n",
        "    x1, y1, x2, y2 = 0, 0, 0, 0 # ì´ˆê¸°í™”\n",
        "    for idx, cls in enumerate(box_class):\n",
        "        if int(cls) == step: #detect_clsë¥¼ stepìœ¼ë¡œ ëŒ€ì²´\n",
        "            x1, y1, x2, y2 = boxes[idx]\n",
        "            cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (225, 0, 0), 2)\n",
        "\n",
        "    image, xyz_list_list, status_num = grab_release(image, yolo_model, hand_list, lstm_model, detect_list[step], hand_cls, length, xyz_list_list, status_num) #detect_list[step]ì¶”ê°€, detect_clsì‚­ì œ\n",
        "    \n",
        "    return image, xyz_list_list, status_num, x1, y1, x2, y2\n",
        "\n",
        "def overlay_text(image, text, position, font_path, font_size, color):\n",
        "    return putText_korean(image, text, position, font_path, font_size, color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cap_device = 0\n",
        "cap = cv2.VideoCapture(cap_device)  # for Mac\n",
        "# cap = cv2.VideoCapture(0)  # for Windows\n",
        "lstm_model.eval()\n",
        "status_num = -1\n",
        "step = 0\n",
        "wait_frames = 60\n",
        "cnt_frames = 0\n",
        "xyz_list_list = []\n",
        "detect_list = [0, 13, 4, 5, 3] #ê°ì²´ ë²ˆí˜¸ì¸ detect_listì¶”ê°€\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    image = cv2.resize(frame, (640, 640))\n",
        "    # detect_cls = step ì‚­ì œ\n",
        "    if step <= len(detect_list) -1: #stepì´ 5ì´ìƒì´ë©´ list index out of rangeì˜¤ë¥˜ ë°œìƒ\n",
        "        image, xyz_list_list, status_num, x1, y1, x2, y2 = detect_and_process(\n",
        "            image, step_model, yolo_model, hand_list, lstm_model, step, detect_list ,hand_cls, length, xyz_list_list, status_num) # detect_clsì‚­ì œ, step, detect_listì¶”ê°€\n",
        "    \n",
        "    # ê° ë‹¨ê³„ë³„ í…ìŠ¤íŠ¸ ì§€ì • ë¡œì§\n",
        "    if step == 0:\n",
        "        text = 'íŒŒë€ ë‹¤ë¦¬ë¥¼ ì§‘ì–´ ì˜¬ë¦¬ì„¸ìš”.' if status_num != 1 else 'ë¹¨ê°„ ì› ìœ„ì— íŒŒë€ ë‹¤ë¦¬ë¥¼ ì˜¬ë ¤ë†“ìœ¼ì„¸ìš”.'\n",
        "    elif step == 1:\n",
        "        text = 'ë…¸ë€ ë‹¤ë¦¬ë¥¼ ì§‘ì–´ ì˜¬ë¦¬ì„¸ìš”.' if status_num != 1 else 'íŒŒë€ìƒ‰ ë‹¤ë¦¬ì˜ ì™¼ìª½ì— ë…¸ë€ìƒ‰ ë‹¤ë¦¬ë¥¼ ë†“ìœ¼ì„¸ìš”.'\n",
        "    elif step == 2:\n",
        "        text = 'ì´ˆë¡ìƒ‰ ì›ì„ ì§‘ì–´ ì˜¬ë¦¬ì„¸ìš”.' if status_num != 1 else 'ë…¸ë€ìƒ‰ ë‹¤ë¦¬ ë°‘ì— ì´ˆë¡ìƒ‰ ì›ì„ ë„£ì–´ì£¼ì„¸ìš”.'\n",
        "    elif step == 3:\n",
        "        text = 'ì´ˆë¡ìƒ‰ íë¸Œë¥¼ ì§‘ì–´ ì˜¬ë¦¬ì„¸ìš”.' if status_num != 1 else 'ë…¸ë€ìƒ‰ ë‹¤ë¦¬ì˜ ì˜¤ë¥¸ìª½ ìœ„ì— ì´ˆë¡ìƒ‰ íë¸Œë¥¼ ì˜¬ë ¤ë†“ìœ¼ì„¸ìš”.'\n",
        "    elif step == 4:\n",
        "        text = 'íŒŒë€ìƒ‰ ë¶€ì±„ê¼´ì„ ì§‘ì–´ ì˜¬ë¦¬ì„¸ìš”.' if status_num != 1 else 'ì´ˆë¡ìƒ‰ íë¸Œì˜ ì™¼ìª½ì— íŒŒë€ìƒ‰ ë¶€ì±„ê¼´ì„ ë†“ìœ¼ì„¸ìš”.'\n",
        "    else:\n",
        "        x1, y1, x2, y2 = 0, 0, 0, 0 # ì´ˆê¸°í™”\n",
        "        text = 'ì™„ë£Œ!'\n",
        "                \n",
        "    if x1 != 0 and y1 != 0 and x2 != 0 and y2 != 0:\n",
        "        text = 'ì°¸ ì˜í–ˆì–´ìš”!'\n",
        "        if cnt_frames < wait_frames:\n",
        "            cnt_frames += 1\n",
        "        else:\n",
        "            step += 1\n",
        "            cnt_frames = 0\n",
        "            status_num = -1\n",
        "    \n",
        "    image = overlay_text(image, text, position, font_path, font_size, color)\n",
        "\n",
        "    cv2.imshow('frame', image)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "    \n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "cv2.waitKey()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
