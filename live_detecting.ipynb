{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minjae0501/yolo_block/blob/master/lstm_yolo_detecting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cECJ3kW1kWsW"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCnOheVr-qBn",
        "outputId": "553d50a6-2eff-47a4-b849-d1a3a64986e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'yolo_block'...\n",
            "remote: Enumerating objects: 3012, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 3012 (delta 3), reused 13 (delta 1), pack-reused 2993\u001b[K\n",
            "Receiving objects: 100% (3012/3012), 117.88 MiB | 50.70 MiB/s, done.\n",
            "Resolving deltas: 100% (50/50), done.\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/minjae0501/yolo_block.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6AKCg3M-pCz",
        "outputId": "919a6b1f-e3a5-4469-bc07-57a8d523aa5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.227 🚀 Python-3.9.13 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce RTX 2070, 8192MiB)\n",
            "Setup complete ✅ (8 CPUs, 15.9 GB RAM, 208.7/232.3 GB disk)\n"
          ]
        }
      ],
      "source": [
        "# %pip install ultralytics\n",
        "# %pip install mediapipe\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Na8wp-xB-pC2"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from ultralytics import YOLO\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import ImageFont, ImageDraw, Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIxePsio-pC3",
        "outputId": "62894ebe-f1c2-41d5-f528-f1c98b44431c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available() == True:\n",
        "    device = 'cuda:0'\n",
        "else:\n",
        "    device = 'cpu'\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, seq_list):\n",
        "        self.X = []\n",
        "        self.y = []\n",
        "        for dic in seq_list:\n",
        "            self.y.append(dic['key'])\n",
        "            self.X.append(dic['value'])\n",
        "    def __getitem__(self, index):\n",
        "        data = self.X[index]\n",
        "        label = self.y[index]\n",
        "        return torch.Tensor(np.array(data)), torch.tensor(np.array(int(label)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ver 9\n",
        "class hand_LSTM(nn.Module):\n",
        "    def __init__(self, num_layers=1):\n",
        "        super(hand_LSTM, self).__init__()\n",
        "        \"\"\"\n",
        "        LayerNorm(): RNN과 LSTM에 적합 \n",
        "        - LSTM과 같은 순환 신경망에서는 시간에 따른 의존성 때문에 배치 정규화가 잘 작동하지 않을 수 있다.\n",
        "        - 반면 레이어 정규화는 시간적 의존성에 영향을 받지 않아 RNN과 LSTM에 더 적합하다.\n",
        "        \"\"\"\n",
        "        # bidirectional -> 양방향 lstm: 시퀀스 데이터를 순방향과 역방향 모두 학습\n",
        "        self.lstm1 = nn.LSTM(67, 128, num_layers, batch_first=True, bidirectional=True)\n",
        "        # lstm layer 정규화 사용, 양방향이기 때문에 256개 \n",
        "        self.layer_norm1 = nn.LayerNorm(256)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        \n",
        "        self.lstm2 = nn.LSTM(256, 64, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.layer_norm2 = nn.LayerNorm(128)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "        \n",
        "        self.lstm3 = nn.LSTM(128, 32, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.layer_norm3 = nn.LayerNorm(64)\n",
        "        self.dropout3 = nn.Dropout(0.1)\n",
        "        \n",
        "        self.attention = nn.Linear(64, 1)\n",
        "        self.fc = nn.Linear(64, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm1(x)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        \n",
        "        x, _ = self.lstm2(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        \n",
        "        x, _ = self.lstm3(x)\n",
        "        x = self.layer_norm3(x)\n",
        "        x = self.dropout3(x)\n",
        "        \n",
        "        # Attention 메커니즘\n",
        "        attention_weights = torch.softmax(self.attention(x), dim=1)\n",
        "        x = torch.sum(attention_weights * x, dim=1)\n",
        "        \n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def grab_release(image, yolo_model, hand_list , lstm_model, detect_cls, hand_cls, length, xyz_list_list, status_num):\n",
        "    mp_hands, hands, mp_drawing = hand_list[0], hand_list[1], hand_list[2]\n",
        "\n",
        "    # YOLO 객체 감지\n",
        "    box_results = yolo_model.predict(image, conf = 0.6, verbose=False, show = False)\n",
        "    boxes = box_results[0].boxes.xyxy.cpu()\n",
        "    box_class = box_results[0].boxes.cls.cpu().tolist()\n",
        "\n",
        "    x1, y1, x2, y2 = 0, 0, 0, 0\n",
        "    hx1, hy1, hx2, hy2 = 0,0,0,0\n",
        "    for idx, cls in enumerate(box_class):\n",
        "        if int(cls) == detect_cls:\n",
        "            x1, y1, x2, y2 = boxes[idx]\n",
        "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
        "            cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "        elif int(cls) == hand_cls:\n",
        "            hx1, hy1, hx2, hy2 = boxes[idx]\n",
        "            hx1, hy1, hx2, hy2 = int(hx1), int(hy1), int(hx2), int(hy2)\n",
        "            cv2.rectangle(image, (int(hx1), int(hy1)), (int(hx2), int(hy2)), (0, 0, 255), 2)\n",
        "    \n",
        "    #mediapipe\n",
        "    results = hands.process(image)\n",
        "    xyz_list = []\n",
        "    if results.multi_hand_landmarks:\n",
        "        for x_y_z in results.multi_hand_landmarks:\n",
        "            for landmark in x_y_z.landmark:\n",
        "                xyz_list.append(landmark.x) # *10 삭제\n",
        "                xyz_list.append(landmark.y)\n",
        "                xyz_list.append(landmark.z) \n",
        "                \n",
        "\n",
        "        xyz_list.append(abs(x1-hx1)/640) # /640 추가\n",
        "        xyz_list.append(abs(x2-hx2)/640)\n",
        "        xyz_list.append(abs(y1-hy1)/640)\n",
        "        xyz_list.append(abs(y2-hy2)/640)\n",
        "\n",
        "        if x1 != 0 and y1 != 0 and x2 != 0 and y2 != 0 and hx1 != 0 and hy1 != 0 and hx2 != 0 and hy2 != 0:\n",
        "            xyz_list_list.append(xyz_list)# 객체와 손이 인식되면\n",
        "\n",
        "        for hand_landmarks in results.multi_hand_landmarks:\n",
        "              with torch.no_grad():\n",
        "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "    \n",
        "    if len(xyz_list_list) == length:\n",
        "        dataset = []\n",
        "        dataset.append({'key': 0, 'value': xyz_list_list})\n",
        "        dataset = MyDataset(dataset)\n",
        "        dataset = DataLoader(dataset)\n",
        "        xyz_list_list = []\n",
        "        for data, label in dataset:\n",
        "            data = data.to(device)\n",
        "            with torch.no_grad():\n",
        "                result = lstm_model(data)\n",
        "                _, out = torch.max(result, 1)\n",
        "                status_num = out.item()\n",
        "                # if out.item() == 0: status = 'Release'\n",
        "                # else: status = 'Grab'\n",
        "\n",
        "    # cv2.putText(image, status, (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1.5, (0,0, 225), 2)\n",
        "\n",
        "    return image, xyz_list_list, status_num\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 한글폰트 출력\n",
        "def putText_korean(img, text, position, font_path, font_size, color):\n",
        "    img_pil = Image.fromarray(img)\n",
        "    draw = ImageDraw.Draw(img_pil)\n",
        "    font = ImageFont.truetype(font_path, font_size)\n",
        "    draw.text(position, text, font=font, fill=color)\n",
        "    return np.array(img_pil)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yaefZpU-pC4",
        "outputId": "4c04ad5d-8c10-4c5d-df5b-b09a6bc7ed68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "모델이 성공적으로 불러와졌습니다.\n"
          ]
        }
      ],
      "source": [
        "# YOLO 객체 감지 모델 초기화\n",
        "best_model = './block_model/block_best_02.pt'\n",
        "yolo_model = YOLO(best_model)\n",
        "\n",
        "# YOLO 스탭 탐지 모델\n",
        "step_best_model = './block_model/truck_best_04.pt'\n",
        "step_model = YOLO(step_best_model)\n",
        "\n",
        "# lstm모델 불러오기\n",
        "model_path = './lstm_pth/more_data_lstm_model_ver9.pth'\n",
        "lstm_model = hand_LSTM().to(device)\n",
        "lstm_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "lstm_model.eval()\n",
        "print(\"모델이 성공적으로 불러와졌습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# mediapipe 손 감지 모듈 초기화\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.3)\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "hand_list = [mp_hands, hands, mp_drawing]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 손, dataset길이 설정\n",
        "hand_cls = 6\n",
        "length = 10\n",
        "\n",
        "# 한글 텍스트를 추가할 위치, 폰트경로, 폰트크기, 색상설정\n",
        "position = (15, 40) # 텍스트를 출력할 위치\n",
        "font_path = './font/KCC-Ganpan.ttf' # 한글 폰트 파일 경로\n",
        "font_size = 30 # 폰트 크기\n",
        "color = (0, 0, 255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 실시간 안내 system 함수화\n",
        "def detect_and_process(image, step_model, yolo_model, hand_list, lstm_model, step, detect_list, hand_cls, length, xyz_list_list, status_num): # detect_cls삭제, step, detect_list추가\n",
        "    box_results = step_model.predict(image, conf = 0.6, verbose=False, show = False)\n",
        "    boxes = box_results[0].boxes.xyxy.cpu()\n",
        "    box_class = box_results[0].boxes.cls.cpu().tolist()\n",
        "    \n",
        "    x1, y1, x2, y2 = 0, 0, 0, 0 # 초기화\n",
        "    for idx, cls in enumerate(box_class):\n",
        "        if int(cls) == step: #detect_cls를 step으로 대체\n",
        "            x1, y1, x2, y2 = boxes[idx]\n",
        "            cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (225, 0, 0), 2)\n",
        "\n",
        "    image, xyz_list_list, status_num = grab_release(image, yolo_model, hand_list, lstm_model, detect_list[step], hand_cls, length, xyz_list_list, status_num) #detect_list[step]추가, detect_cls삭제\n",
        "    \n",
        "    return image, xyz_list_list, status_num, x1, y1, x2, y2\n",
        "\n",
        "def overlay_text(image, text, position, font_path, font_size, color):\n",
        "    return putText_korean(image, text, position, font_path, font_size, color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cap_device = 0\n",
        "cap = cv2.VideoCapture(cap_device)  # for Mac\n",
        "# cap = cv2.VideoCapture(0)  # for Windows\n",
        "lstm_model.eval()\n",
        "status_num = -1\n",
        "step = 0\n",
        "wait_frames = 60\n",
        "cnt_frames = 0\n",
        "xyz_list_list = []\n",
        "detect_list = [0, 13, 4, 5, 3] #객체 번호인 detect_list추가\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    image = cv2.resize(frame, (640, 640))\n",
        "    # detect_cls = step 삭제\n",
        "    if step <= len(detect_list) -1: #step이 5이상이면 list index out of range오류 발생\n",
        "        image, xyz_list_list, status_num, x1, y1, x2, y2 = detect_and_process(\n",
        "            image, step_model, yolo_model, hand_list, lstm_model, step, detect_list ,hand_cls, length, xyz_list_list, status_num) # detect_cls삭제, step, detect_list추가\n",
        "    \n",
        "    # 각 단계별 텍스트 지정 로직\n",
        "    if step == 0:\n",
        "        text = '파란 다리를 집어 올리세요.' if status_num != 1 else '빨간 원 위에 파란 다리를 올려놓으세요.'\n",
        "    elif step == 1:\n",
        "        text = '노란 다리를 집어 올리세요.' if status_num != 1 else '파란색 다리의 왼쪽에 노란색 다리를 놓으세요.'\n",
        "    elif step == 2:\n",
        "        text = '초록색 원을 집어 올리세요.' if status_num != 1 else '노란색 다리 밑에 초록색 원을 넣어주세요.'\n",
        "    elif step == 3:\n",
        "        text = '초록색 큐브를 집어 올리세요.' if status_num != 1 else '노란색 다리의 오른쪽 위에 초록색 큐브를 올려놓으세요.'\n",
        "    elif step == 4:\n",
        "        text = '파란색 부채꼴을 집어 올리세요.' if status_num != 1 else '초록색 큐브의 왼쪽에 파란색 부채꼴을 놓으세요.'\n",
        "    else:\n",
        "        x1, y1, x2, y2 = 0, 0, 0, 0 # 초기화\n",
        "        text = '완료!'\n",
        "                \n",
        "    if x1 != 0 and y1 != 0 and x2 != 0 and y2 != 0:\n",
        "        text = '참 잘했어요!'\n",
        "        if cnt_frames < wait_frames:\n",
        "            cnt_frames += 1\n",
        "        else:\n",
        "            step += 1\n",
        "            cnt_frames = 0\n",
        "            status_num = -1\n",
        "    \n",
        "    image = overlay_text(image, text, position, font_path, font_size, color)\n",
        "\n",
        "    cv2.imshow('frame', image)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "    \n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "cv2.waitKey()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
