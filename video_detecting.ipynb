{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minjae0501/yolo_block/blob/master/lstm_yolo_detecting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cECJ3kW1kWsW"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCnOheVr-qBn",
        "outputId": "553d50a6-2eff-47a4-b849-d1a3a64986e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'yolo_block'...\n",
            "remote: Enumerating objects: 3012, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 3012 (delta 3), reused 13 (delta 1), pack-reused 2993\u001b[K\n",
            "Receiving objects: 100% (3012/3012), 117.88 MiB | 50.70 MiB/s, done.\n",
            "Resolving deltas: 100% (50/50), done.\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/minjae0501/yolo_block.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6AKCg3M-pCz",
        "outputId": "919a6b1f-e3a5-4469-bc07-57a8d523aa5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.227 🚀 Python-3.9.13 torch-1.12.1+cu116 CUDA:0 (NVIDIA GeForce RTX 2070, 8192MiB)\n",
            "Setup complete ✅ (8 CPUs, 15.9 GB RAM, 210.8/232.3 GB disk)\n"
          ]
        }
      ],
      "source": [
        "# %pip install ultralytics\n",
        "# %pip install mediapipe\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Na8wp-xB-pC2"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "from ultralytics import YOLO\n",
        "import numpy as np\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIxePsio-pC3",
        "outputId": "62894ebe-f1c2-41d5-f528-f1c98b44431c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available() == True:\n",
        "    device = 'cuda:0'\n",
        "else:\n",
        "    device = 'cpu'\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lX-RhFX5-pC3"
      },
      "outputs": [],
      "source": [
        "# ver 9\n",
        "class hand_LSTM(nn.Module):\n",
        "    def __init__(self, num_layers=1):\n",
        "        super(hand_LSTM, self).__init__()\n",
        "        \"\"\"\n",
        "        LayerNorm(): RNN과 LSTM에 적합 \n",
        "        - LSTM과 같은 순환 신경망에서는 시간에 따른 의존성 때문에 배치 정규화가 잘 작동하지 않을 수 있다.\n",
        "        - 반면 레이어 정규화는 시간적 의존성에 영향을 받지 않아 RNN과 LSTM에 더 적합하다.\n",
        "        \"\"\"\n",
        "        # bidirectional -> 양방향 lstm: 시퀀스 데이터를 순방향과 역방향 모두 학습\n",
        "        self.lstm1 = nn.LSTM(67, 128, num_layers, batch_first=True, bidirectional=True)\n",
        "        # lstm layer 정규화 사용, 양방향이기 때문에 256개 \n",
        "        self.layer_norm1 = nn.LayerNorm(256)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        \n",
        "        self.lstm2 = nn.LSTM(256, 64, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.layer_norm2 = nn.LayerNorm(128)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "        \n",
        "        self.lstm3 = nn.LSTM(128, 32, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.layer_norm3 = nn.LayerNorm(64)\n",
        "        self.dropout3 = nn.Dropout(0.1)\n",
        "        \n",
        "        self.attention = nn.Linear(64, 1)\n",
        "        self.fc = nn.Linear(64, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm1(x)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        \n",
        "        x, _ = self.lstm2(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        \n",
        "        x, _ = self.lstm3(x)\n",
        "        x = self.layer_norm3(x)\n",
        "        x = self.dropout3(x)\n",
        "        \n",
        "        # Attention 메커니즘\n",
        "        attention_weights = torch.softmax(self.attention(x), dim=1)\n",
        "        x = torch.sum(attention_weights * x, dim=1)\n",
        "        \n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yaefZpU-pC4",
        "outputId": "4c04ad5d-8c10-4c5d-df5b-b09a6bc7ed68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "모델이 성공적으로 불러와졌습니다.\n"
          ]
        }
      ],
      "source": [
        "# 모델 불러오기\n",
        "model_path = './lstm_pth/more_data_lstm_model_ver9.pth'\n",
        "\n",
        "lstm_model = hand_LSTM().to(device)\n",
        "lstm_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "lstm_model.eval()\n",
        "print(\"모델이 성공적으로 불러와졌습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZtEVOlDp-pC4"
      },
      "outputs": [],
      "source": [
        "# YOLO 객체 감지 모델 초기화\n",
        "best_model = './block_model/block_best_02.pt'\n",
        "# best_model = 'best.pt'\n",
        "yolo_model = YOLO(best_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GvvqZ0SW-pC5"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, seq_list):\n",
        "        self.X = []\n",
        "        self.y = []\n",
        "        for dic in seq_list:\n",
        "            self.y.append(dic['key'])\n",
        "            self.X.append(dic['value'])\n",
        "    def __getitem__(self, index):\n",
        "        data = self.X[index]\n",
        "        label = self.y[index]\n",
        "        return torch.Tensor(np.array(data)), torch.tensor(np.array(int(label)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RYjUi-6-pC6",
        "outputId": "781dee65-84e9-47c9-a9ce-98557c48f84a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "저장된 frame의 개수 673\n"
          ]
        }
      ],
      "source": [
        "length = 20\n",
        "interval = 1\n",
        "detect_cls = 1\n",
        "\n",
        "video_path = './data/video_data/val/test.mp4'\n",
        "# cv2.destroyAllWindows()\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "img_list = []\n",
        "if cap.isOpened():\n",
        "    cnt = 0\n",
        "    while True:\n",
        "        ret , img = cap.read()\n",
        "        if ret:\n",
        "            img = cv2.resize(img, (640, 640))\n",
        "            if cnt == interval:\n",
        "                img_list.append(img)\n",
        "                cnt =0\n",
        "            # cv2.imshow('test_video', img)\n",
        "            # cv2.waitKey(1)\n",
        "            cnt += 1\n",
        "\n",
        "        else: break\n",
        "cap.release()\n",
        "# cv2.destroyAllWindows()\n",
        "\n",
        "print('저장된 frame의 개수 {}'.format(len(img_list)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGlQQ7Ky-pC6",
        "outputId": "e842ba4b-5e45-437c-fb89-4463b522a4c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "시퀀스 데이터 분석 중..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 673/673 [00:22<00:00, 30.39it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "lstm_model.eval()\n",
        "out_img_list = []\n",
        "dataset = []\n",
        "status = 'None'\n",
        "\n",
        "detect_cls = 0\n",
        "hand_cls = 6\n",
        "\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5)\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "print('시퀀스 데이터 분석 중..')\n",
        "xyz_list_list = []\n",
        "\n",
        "for img in tqdm(img_list):\n",
        "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    results = hands.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    xyz_list = []\n",
        "    filtered_boxes, filtered_box_class = [], []\n",
        "\n",
        "    if not results.multi_hand_landmarks:\n",
        "          continue\n",
        "\n",
        "    for x_y_z in results.multi_hand_landmarks:\n",
        "          for landmark in x_y_z.landmark:\n",
        "            xyz_list.append(landmark.x)\n",
        "            xyz_list.append(landmark.y)\n",
        "            xyz_list.append(landmark.z)\n",
        "\n",
        "          mp_drawing.draw_landmarks(img, x_y_z, mp_hands.HAND_CONNECTIONS)\n",
        "\n",
        "    # YOLO 박스\n",
        "    box_results = yolo_model.predict(img, conf = 0.6, verbose=False, show = False)\n",
        "    box_results = box_results[0].boxes\n",
        "    boxes = box_results.xyxy.cpu().tolist()\n",
        "    box_class = box_results.cls.cpu().tolist()\n",
        "\n",
        "\n",
        "    x1, y1, x2, y2 = 0, 0, 0, 0\n",
        "    hx1, hy1, hx2, hy2 = 0,0,0,0\n",
        "    for idx, cls in enumerate(box_class):\n",
        "        if int(cls) == detect_cls:\n",
        "            x1, y1, x2, y2 = boxes[idx]\n",
        "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
        "            cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "        elif int(cls) == hand_cls:\n",
        "            hx1, hy1, hx2, hy2 = boxes[idx]\n",
        "            hx1, hy1, hx2, hy2 = int(hx1), int(hy1), int(hx2), int(hy2)\n",
        "            cv2.rectangle(img, (int(hx1), int(hy1)), (int(hx2), int(hy2)), (0, 0, 255), 2)\n",
        "\n",
        "    xyz_list.append(abs(x1-hx1)/640) \n",
        "    xyz_list.append(abs(x2-hx2)/640)\n",
        "    xyz_list.append(abs(y1-hy1)/640)\n",
        "    xyz_list.append(abs(y2-hy2)/640)\n",
        "\n",
        "    xyz_list_list.append(xyz_list)\n",
        "\n",
        "    if len(xyz_list_list) == length:\n",
        "        dataset = []\n",
        "        dataset.append({'key': 0, 'value': xyz_list_list})\n",
        "        dataset = MyDataset(dataset)\n",
        "        dataset = DataLoader(dataset)\n",
        "        xyz_list_list = []\n",
        "        for data, label in dataset:\n",
        "            data = data.to(device)\n",
        "            with torch.no_grad():\n",
        "                result = lstm_model(data)\n",
        "                _, out = torch.max(result, 1)\n",
        "                if out.item() == 0: status = 'Release'\n",
        "                else: status = 'Grab'\n",
        "\n",
        "    cv2.putText(img, status, (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1.5, (0,0, 225), 2)\n",
        "    out_img_list.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Gvk1PjxX-pC6"
      },
      "outputs": [],
      "source": [
        "filename = './data/video_data/output/video_out.mp4'\n",
        "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
        "fps = 30\n",
        "frameSize = (640,640)\n",
        "isColor = True\n",
        "out = cv2.VideoWriter(filename, fourcc, fps, frameSize, isColor)\n",
        "for out_img in out_img_list:\n",
        "    # print(out_img)\n",
        "    out.write(out_img)\n",
        "\n",
        "out.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FH2UhLBz-pC7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
