{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minjae0501/yolo_block/blob/master/lstm_yolo_detecting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cECJ3kW1kWsW"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCnOheVr-qBn",
        "outputId": "553d50a6-2eff-47a4-b849-d1a3a64986e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'yolo_block'...\n",
            "remote: Enumerating objects: 3012, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 3012 (delta 3), reused 13 (delta 1), pack-reused 2993\u001b[K\n",
            "Receiving objects: 100% (3012/3012), 117.88 MiB | 50.70 MiB/s, done.\n",
            "Resolving deltas: 100% (50/50), done.\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/minjae0501/yolo_block.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6AKCg3M-pCz",
        "outputId": "919a6b1f-e3a5-4469-bc07-57a8d523aa5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.220 üöÄ Python-3.9.0 torch-2.1.1 CPU (Apple M1 Pro)\n",
            "Setup complete ‚úÖ (10 CPUs, 16.0 GB RAM, 313.4/460.4 GB disk)\n"
          ]
        }
      ],
      "source": [
        "# %pip install ultralytics\n",
        "# %pip install mediapipe\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Na8wp-xB-pC2"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from ultralytics import YOLO\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import ImageFont, ImageDraw, Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIxePsio-pC3",
        "outputId": "62894ebe-f1c2-41d5-f528-f1c98b44431c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available() == True:\n",
        "    device = 'cuda:0'\n",
        "else:\n",
        "    device = 'cpu'\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, seq_list):\n",
        "        self.X = []\n",
        "        self.y = []\n",
        "        for dic in seq_list:\n",
        "            self.y.append(dic['key'])\n",
        "            self.X.append(dic['value'])\n",
        "    def __getitem__(self, index):\n",
        "        data = self.X[index]\n",
        "        label = self.y[index]\n",
        "        return torch.Tensor(np.array(data)), torch.tensor(np.array(int(label)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lX-RhFX5-pC3"
      },
      "outputs": [],
      "source": [
        "class hand_LSTM(nn.Module):\n",
        "    def __init__(self, num_layers=1):\n",
        "        super(hand_LSTM, self).__init__()\n",
        "        # LSTM Î†àÏù¥Ïñ¥ Î∞è ÎìúÎ°≠ÏïÑÏõÉ Î†àÏù¥Ïñ¥ Ï†ïÏùò\n",
        "        self.lstm1 = nn.LSTM(67, 128, num_layers, batch_first=True) \n",
        "       \n",
        "        self.lstm4 = nn.LSTM(128, 64, num_layers, batch_first=True)\n",
        "        self.dropout4 = nn.Dropout(0.3)\n",
        "        \n",
        "        self.lstm5 = nn.LSTM(64, 32, num_layers, batch_first=True)\n",
        "        self.dropout5 = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(32, 2)  # ÏµúÏ¢Ö Ï∂úÎ†•ÏùÑ ÏúÑÌïú ÏÑ†Ìòï Î†àÏù¥Ïñ¥\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ÏàúÏ†ÑÌåå Ï†ïÏùò\n",
        "        x, _ = self.lstm1(x)\n",
        "        x, _ = self.lstm4(x)\n",
        "        x = self.dropout4(x)\n",
        "        x, _ = self.lstm5(x)\n",
        "        x = self.dropout5(x)\n",
        "        x = self.fc(x[:, -1, :])  # ÏµúÏ¢Ö ÏãúÌÄÄÏä§Ïùò Ï∂úÎ†•Îßå ÏÇ¨Ïö©\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def grab_release(image, yolo_model, hand_list , lstm_model, detect_cls, hand_cls, length, xyz_list_list, status, status_num):\n",
        "    mp_hands, hands, mp_drawing = hand_list[0], hand_list[1], hand_list[2]\n",
        "\n",
        "    # YOLO Í∞ùÏ≤¥ Í∞êÏßÄ\n",
        "    box_results = yolo_model.predict(image, conf = 0.6, verbose=False, show = False)\n",
        "    boxes = box_results[0].boxes.xyxy.cpu()\n",
        "    box_class = box_results[0].boxes.cls.cpu().tolist()\n",
        "\n",
        "    x1, y1, x2, y2 = 0, 0, 0, 0\n",
        "    hx1, hy1, hx2, hy2 = 0,0,0,0\n",
        "    for idx, cls in enumerate(box_class):\n",
        "        if int(cls) == detect_cls:\n",
        "            x1, y1, x2, y2 = boxes[idx]\n",
        "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
        "            cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "        elif int(cls) == hand_cls:\n",
        "            hx1, hy1, hx2, hy2 = boxes[idx]\n",
        "            hx1, hy1, hx2, hy2 = int(hx1), int(hy1), int(hx2), int(hy2)\n",
        "            cv2.rectangle(image, (int(hx1), int(hy1)), (int(hx2), int(hy2)), (0, 0, 255), 2)\n",
        "    \n",
        "    #mediapipe\n",
        "    results = hands.process(image)\n",
        "    xyz_list = []\n",
        "    if results.multi_hand_landmarks:\n",
        "        for x_y_z in results.multi_hand_landmarks:\n",
        "            for landmark in x_y_z.landmark:\n",
        "                xyz_list.append(landmark.x * 10)\n",
        "                xyz_list.append(landmark.y * 10)\n",
        "                xyz_list.append(landmark.z * 10) # \n",
        "                \n",
        "\n",
        "        xyz_list.append(abs(x1-hx1))\n",
        "        xyz_list.append(abs(x2-hx2))\n",
        "        xyz_list.append(abs(y1-hy1))\n",
        "        xyz_list.append(abs(y2-hy2))\n",
        "\n",
        "        if x1 != 0 and y1 != 0 and x2 != 0 and y2 != 0 and hx1 != 0 and hy1 != 0 and hx2 != 0 and hy2 != 0:\n",
        "            xyz_list_list.append(xyz_list)# Í∞ùÏ≤¥ÏôÄ ÏÜêÏù¥ Í≤πÏ≥êÏïº \n",
        "\n",
        "        for hand_landmarks in results.multi_hand_landmarks:\n",
        "              with torch.no_grad():\n",
        "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "    \n",
        "    if len(xyz_list_list) == length:\n",
        "        dataset = []\n",
        "        dataset.append({'key': 0, 'value': xyz_list_list})\n",
        "        dataset = MyDataset(dataset)\n",
        "        dataset = DataLoader(dataset)\n",
        "        xyz_list_list = []\n",
        "        for data, label in dataset:\n",
        "            data = data.to(device)\n",
        "            with torch.no_grad():\n",
        "                result = lstm_model(data)\n",
        "                _, out = torch.max(result, 1)\n",
        "                status_num = out.item()\n",
        "                if out.item() == 0: status = 'Release'\n",
        "                else: status = 'Grab'\n",
        "\n",
        "    # cv2.putText(image, status, (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1.5, (0,0, 225), 2)\n",
        "\n",
        "    return image, xyz_list_list, status, status_num\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ÌïúÍ∏ÄÌè∞Ìä∏ Ï∂úÎ†•\n",
        "def putText_korean(img, text, position, font_path, font_size, color):\n",
        "    img_pil = Image.fromarray(img)\n",
        "    draw = ImageDraw.Draw(img_pil)\n",
        "    font = ImageFont.truetype(font_path, font_size)\n",
        "    draw.text(position, text, font=font, fill=color)\n",
        "    return np.array(img_pil)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ïã§ÏãúÍ∞Ñ ÏïàÎÇ¥ system Ìï®ÏàòÌôî\n",
        "def process_step(image, step_model, lstm_model, yolo_model, hand_list, hand_cls, length, xyz_list_list, step_info):\n",
        "    box_results = step_model.predict(image, conf=0.6, verbose=False, show=False)\n",
        "    boxes = box_results[0].boxes.xyxy.cpu()\n",
        "    box_class = box_results[0].boxes.cls.cpu().tolist()\n",
        "    \n",
        "    # Í∞ùÏ≤¥ Í∞êÏßÄ Î∞è ÌëúÏãú\n",
        "    x1, y1, x2, y2 = 0, 0, 0, 0\n",
        "    for idx, cls in enumerate(box_class):\n",
        "        if int(cls) == step_info['step']:\n",
        "            x1, y1, x2, y2 = map(int, boxes[idx])\n",
        "            cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "    \n",
        "    text = step_info['default_text']\n",
        "    if step_info['status'] == 'None':\n",
        "        # ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏\n",
        "        image, xyz_list_list, status, status_num = grab_release(image, yolo_model, hand_list, lstm_model, step_info['detect_cls'], hand_cls, length, xyz_list_list, step_info['status'], step_info['status_num'])\n",
        "        \n",
        "        if status_num == 1:\n",
        "            text = step_info['action_text']\n",
        "            step_info['status'] = 'Detected'\n",
        "        elif x1 != 0 and y1 != 0 and x2 != 0 and y2 != 0:\n",
        "            step_info['cnt_frames'] += 1\n",
        "            if step_info['cnt_frames'] >= step_info['wait_frames']:\n",
        "                step_info['cnt_frames'] = 0\n",
        "                step_info['status'] = 'Completed'\n",
        "\n",
        "    elif step_info['status'] == 'Completed':\n",
        "        text = 'ÏûòÌñàÏñ¥Ïöî!'\n",
        "        return True, image, xyz_list_list, step_info, text  # ÌòÑÏû¨ Îã®Í≥Ñ ÏôÑÎ£å\n",
        "\n",
        "    return False, image, xyz_list_list, step_info, text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yaefZpU-pC4",
        "outputId": "4c04ad5d-8c10-4c5d-df5b-b09a6bc7ed68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Î™®Îç∏Ïù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Î∂àÎü¨ÏôÄÏ°åÏäµÎãàÎã§.\n"
          ]
        }
      ],
      "source": [
        "# lstmÎ™®Îç∏ Î∂àÎü¨Ïò§Í∏∞\n",
        "model_path = './lstm_pth/more_data_lstm_model_ver2.pth'\n",
        "\n",
        "# YOLO Í∞ùÏ≤¥ Í∞êÏßÄ Î™®Îç∏ Ï¥àÍ∏∞Ìôî\n",
        "best_model = 'block_best_02.pt'\n",
        "yolo_model = YOLO(best_model)\n",
        "\n",
        "# YOLO Ïä§ÌÉ≠ ÌÉêÏßÄ Î™®Îç∏\n",
        "step_best_model = 'truck_best_03.pt'\n",
        "step_model = YOLO(step_best_model)\n",
        "\n",
        "lstm_model = hand_LSTM().to(device)\n",
        "lstm_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "lstm_model.eval()\n",
        "print(\"Î™®Îç∏Ïù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Î∂àÎü¨ÏôÄÏ°åÏäµÎãàÎã§.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
          ]
        }
      ],
      "source": [
        "# mediapipe ÏÜê Í∞êÏßÄ Î™®Îìà Ï¥àÍ∏∞Ìôî\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.3)\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "hand_list = [mp_hands, hands, mp_drawing]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ÏÜê, datasetÍ∏∏Ïù¥ ÏÑ§Ï†ï\n",
        "hand_cls = 6\n",
        "length = 5\n",
        "\n",
        "# ÌïúÍ∏Ä ÌÖçÏä§Ìä∏Î•º Ï∂îÍ∞ÄÌï† ÏúÑÏπò, Ìè∞Ìä∏Í≤ΩÎ°ú, Ìè∞Ìä∏ÌÅ¨Í∏∞, ÏÉâÏÉÅÏÑ§Ï†ï\n",
        "position = (40, 40) # ÌÖçÏä§Ìä∏Î•º Ï∂úÎ†•Ìï† ÏúÑÏπò\n",
        "font_path = './font/KCC-Ganpan.ttf' # ÌïúÍ∏Ä Ìè∞Ìä∏ ÌååÏùº Í≤ΩÎ°ú\n",
        "font_size = 30 # Ìè∞Ìä∏ ÌÅ¨Í∏∞\n",
        "color = (0, 0, 255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Í∏∞Î≥∏ ÏÑ§Ï†ï Î∞è Ïπ¥Î©îÎùº Ï¥àÍ∏∞Ìôî\n",
        "cap_device = 0\n",
        "cap = cv2.VideoCapture(cap_device)\n",
        "lstm_model.eval()\n",
        "current_step = 0 # ÌòÑÏû¨ ÏßÑÌñâ Ï§ëÏù∏ Îã®Í≥ÑÎ•º Ï∂îÏ†ÅÌïòÎäî Î≥ÄÏàò\n",
        "xyz_list_list = [] # Î≥ÄÏàò Ï¥àÍ∏∞Ìôî\n",
        "\n",
        "steps = [\n",
        "    {'step': 0, 'wait_frames': 60, 'cnt_frames': 0, 'detect_cls': 0, 'default_text': 'ÌååÎûÄ Îã§Î¶¨Î•º ÏßëÏñ¥ Ïò¨Î¶¨ÏÑ∏Ïöî.', 'action_text': 'Îπ®Í∞Ñ Ïõê ÏúÑÏóê ÌååÎûÄ Îã§Î¶¨Î•º Ïò¨Î†§ÎÜìÏúºÏÑ∏Ïöî.', 'status': 'None', 'status_num': -1},\n",
        "    {'step': 1, 'wait_frames': 60, 'cnt_frames': 0, 'detect_cls': 13, 'default_text': 'ÎÖ∏ÎûÄ Îã§Î¶¨Î•º ÏßëÏñ¥ Ïò¨Î¶¨ÏÑ∏Ïöî.', 'action_text': 'ÌååÎûÄÏÉâ Îã§Î¶¨Ïùò ÏôºÏ™ΩÏóê ÎÖ∏ÎûÄÏÉâ Îã§Î¶¨Î•º ÎÜìÏúºÏÑ∏Ïöî.', 'status': 'None', 'status_num': -1},\n",
        "    {'step': 2, 'wait_frames': 60, 'cnt_frames': 0, 'detect_cls': 4, 'default_text': 'Ï¥àÎ°ùÏÉâ ÏõêÏùÑ ÏßëÏñ¥ Ïò¨Î¶¨ÏÑ∏Ïöî.', 'action_text': 'ÎÖ∏ÎûÄÏÉâ Îã§Î¶¨ Î∞ëÏóê Ï¥àÎ°ùÏÉâ ÏõêÏùÑ ÎÑ£Ïñ¥Ï£ºÏÑ∏Ïöî.', 'status': 'None', 'status_num': -1},\n",
        "    {'step': 3, 'wait_frames': 60, 'cnt_frames': 0, 'detect_cls': 5, 'default_text': 'Ï¥àÎ°ùÏÉâ ÌÅêÎ∏åÎ•º ÏßëÏñ¥ Ïò¨Î¶¨ÏÑ∏Ïöî.', 'action_text': 'ÎÖ∏ÎûÄÏÉâ Îã§Î¶¨Ïùò Ïò§Î•∏Ï™Ω ÏúÑÏóê Ï¥àÎ°ùÏÉâ ÌÅêÎ∏åÎ•º Ïò¨Î†§ÎÜìÏúºÏÑ∏Ïöî.', 'status': 'None', 'status_num': -1},\n",
        "    {'step': 4, 'wait_frames': 60, 'cnt_frames': 0, 'detect_cls': 3, 'default_text': 'ÌååÎûÄÏÉâ Î∂ÄÏ±ÑÍº¥ÏùÑ ÏßëÏñ¥ Ïò¨Î¶¨ÏÑ∏Ïöî.', 'action_text': 'Ï¥àÎ°ùÏÉâ ÌÅêÎ∏åÏùò ÏôºÏ™ΩÏóê ÌååÎûÄÏÉâ Î∂ÄÏ±ÑÍº¥ÏùÑ ÎÜìÏúºÏÑ∏Ïöî.', 'status': 'None', 'status_num': -1}\n",
        "]\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    \n",
        "    image = cv2.resize(frame, (640, 640))\n",
        "\n",
        "    step_completed, image, xyz_list_list, step_info, text = process_step(\n",
        "        image, step_model, lstm_model, yolo_model, hand_list, hand_cls, length, xyz_list_list, steps[current_step])\n",
        "\n",
        "    if step_completed:\n",
        "        if current_step < len(steps) - 1:\n",
        "            current_step += 1  # Îã§Ïùå Îã®Í≥ÑÎ°ú Ïù¥Îèô\n",
        "        else:\n",
        "            break  # Î™®Îì† Îã®Í≥Ñ ÏôÑÎ£å\n",
        "\n",
        "    image = putText_korean(image, text, position, font_path, font_size, color)\n",
        "    cv2.imshow('frame', image)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "    \n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "cv2.waitKey()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "FH2UhLBz-pC7"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'cv2' has no attribute 'waitkey'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[32], line 149\u001b[0m\n\u001b[1;32m    147\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    148\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m--> 149\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitkey\u001b[49m()\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'waitkey'"
          ]
        }
      ],
      "source": [
        "cap_device = 0\n",
        "cap = cv2.VideoCapture(cap_device) # for Mac\n",
        "# cap = cv2.VideoCapture(0) # for Windows\n",
        "lstm_model.eval()\n",
        "status = 'None'\n",
        "status_num = -1\n",
        "step = 0\n",
        "wait_frames = 60\n",
        "cnt_frames = 0\n",
        "xyz_list_list = []\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    \n",
        "    image = cv2.resize(frame, (640, 640))\n",
        "\n",
        "    box_results = step_model.predict(image, conf = 0.6, verbose=False, show = False)\n",
        "    boxes = box_results[0].boxes.xyxy.cpu()\n",
        "    box_class = box_results[0].boxes.cls.cpu().tolist()\n",
        "\n",
        "    x1, y1, x2, y2 = 0, 0, 0, 0\n",
        "    for idx, cls in enumerate(box_class):\n",
        "        if int(cls) == step:\n",
        "            x1, y1, x2, y2 = boxes[idx]\n",
        "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
        "            cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (225, 0, 0), 2)\n",
        "    \n",
        "    if step == 0 :\n",
        "        detect_cls = 0\n",
        "\n",
        "        image, xyz_list_list, status, status_num = grab_release(image, yolo_model, hand_list , lstm_model, detect_cls, hand_cls, length, xyz_list_list, status, status_num)\n",
        "\n",
        "        if status_num == 1:\n",
        "            text = 'Îπ®Í∞Ñ Ïõê ÏúÑÏóê ÌååÎûÄ Îã§Î¶¨Î•º Ïò¨Î†§ÎÜìÏúºÏÑ∏Ïöî.'\n",
        "        else:\n",
        "            text = 'ÌååÎûÄ Îã§Î¶¨Î•º ÏßëÏñ¥ Ïò¨Î¶¨ÏÑ∏Ïöî.'\n",
        "        \n",
        "\n",
        "        if x1 != 0 and y1 != 0 and x2 != 0 and y2 != 0:\n",
        "            if cnt_frames != wait_frames:\n",
        "                text = 'ÏûòÌñàÏñ¥Ïöî!'\n",
        "                cnt_frames += 1\n",
        "            else:\n",
        "                step += 1\n",
        "                cnt_frames = 0\n",
        "                status = 'None'\n",
        "                status_num = -1\n",
        "        image = putText_korean(image, text, position, font_path, font_size, color)\n",
        "\n",
        "    elif step == 1:\n",
        "        detect_cls = 13\n",
        "\n",
        "        image, xyz_list_list, status, status_num = grab_release(image, yolo_model, hand_list , lstm_model, detect_cls, hand_cls, length, xyz_list_list, status, status_num)\n",
        "\n",
        "        if status_num == 1:\n",
        "            text = 'ÌååÎûÄÏÉâ Îã§Î¶¨Ïùò ÏôºÏ™ΩÏóê ÎÖ∏ÎûÄÏÉâ Îã§Î¶¨Î•º ÎÜìÏúºÏÑ∏Ïöî.'\n",
        "        else:\n",
        "            text = 'ÎÖ∏ÎûÄ Îã§Î¶¨Î•º ÏßëÏñ¥ Ïò¨Î¶¨ÏÑ∏Ïöî.'\n",
        "\n",
        "        if x1 != 0 and y1 != 0 and x2 != 0 and y2 != 0:\n",
        "            if cnt_frames != wait_frames:\n",
        "                text = 'ÏûòÌñàÏñ¥Ïöî!'\n",
        "                cnt_frames += 1\n",
        "            else:\n",
        "                step += 1\n",
        "                cnt_frames = 0\n",
        "                status = 'None'\n",
        "                status_num = -1\n",
        "\n",
        "        image = putText_korean(image, text, position, font_path, font_size, color)\n",
        "    \n",
        "    elif step == 2:\n",
        "        detect_cls = 4\n",
        "\n",
        "        image, xyz_list_list, status, status_num = grab_release(image, yolo_model, hand_list , lstm_model, detect_cls, hand_cls, length, xyz_list_list, status, status_num)\n",
        "\n",
        "        if status_num == 1:\n",
        "            text = 'ÎÖ∏ÎûÄÏÉâ Îã§Î¶¨ Î∞ëÏóê Ï¥àÎ°ùÏÉâ ÏõêÏùÑ ÎÑ£Ïñ¥Ï£ºÏÑ∏Ïöî.'\n",
        "        else:\n",
        "            text = 'Ï¥àÎ°ùÏÉâ ÏõêÏùÑ ÏßëÏñ¥ Ïò¨Î¶¨ÏÑ∏Ïöî.'\n",
        "\n",
        "        if x1 != 0 and y1 != 0 and x2 != 0 and y2 != 0:\n",
        "            if cnt_frames != wait_frames:\n",
        "                text = 'Ï†ïÎßê Ïûò Ï°∞Î¶ΩÌñàÏñ¥Ïöî!'\n",
        "                cnt_frames += 1\n",
        "            else:\n",
        "                step += 1\n",
        "                cnt_frames = 0\n",
        "                status = 'None'\n",
        "                status_num = -1\n",
        "                \n",
        "        image = putText_korean(image, text, position, font_path, font_size, color)\n",
        "    \n",
        "    elif step == 3:\n",
        "        detect_cls = 5\n",
        "\n",
        "        image, xyz_list_list, status, status_num = grab_release(image, yolo_model, hand_list , lstm_model, detect_cls, hand_cls, length, xyz_list_list, status, status_num)\n",
        "\n",
        "        if status_num == 1:\n",
        "            text = 'ÎÖ∏ÎûÄÏÉâ Îã§Î¶¨Ïùò Ïò§Î•∏Ï™Ω ÏúÑÏóê Ï¥àÎ°ùÏÉâ ÌÅêÎ∏åÎ•º Ïò¨Î†§ÎÜìÏúºÏÑ∏Ïöî.'\n",
        "        else:\n",
        "            text = 'Ï¥àÎ°ùÏÉâ ÌÅêÎ∏åÎ•º ÏßëÏñ¥ Ïò¨Î¶¨ÏÑ∏Ïöî.'\n",
        "\n",
        "        if x1 != 0 and y1 != 0 and x2 != 0 and y2 != 0:\n",
        "            if cnt_frames != wait_frames:\n",
        "                text = 'Ï†ïÎßê Ïûò Ï°∞Î¶ΩÌñàÏñ¥Ïöî!'\n",
        "                cnt_frames += 1\n",
        "            else:\n",
        "                step += 1\n",
        "                cnt_frames = 0\n",
        "                status = 'None'\n",
        "                status_num = -1\n",
        "                \n",
        "        image = putText_korean(image, text, position, font_path, font_size, color)\n",
        "    \n",
        "    elif step == 4:\n",
        "        detect_cls = 3\n",
        "\n",
        "        image, xyz_list_list, status, status_num = grab_release(image, yolo_model, hand_list , lstm_model, detect_cls, hand_cls, length, xyz_list_list, status, status_num)\n",
        "\n",
        "        if status_num == 1:\n",
        "            text = 'Ï¥àÎ°ùÏÉâ ÌÅêÎ∏åÏùò ÏôºÏ™ΩÏóê ÌååÎûÄÏÉâ Î∂ÄÏ±ÑÍº¥ÏùÑ ÎÜìÏúºÏÑ∏Ïöî.'\n",
        "        else:\n",
        "            text = 'ÌååÎûÄÏÉâ Î∂ÄÏ±ÑÍº¥ÏùÑ ÏßëÏñ¥ Ïò¨Î¶¨ÏÑ∏Ïöî.'\n",
        "\n",
        "        if x1 != 0 and y1 != 0 and x2 != 0 and y2 != 0:\n",
        "            if cnt_frames != wait_frames:\n",
        "                text = 'ÏôÑÎ≤ΩÌïòÍ≤å Ìï¥ÎÉàÏñ¥Ïöî!'\n",
        "                # cnt_frames += 1\n",
        "            # else:\n",
        "            #     step += 1\n",
        "            #     cnt_frames = 0\n",
        "            #     status = 'None'\n",
        "            #     status_num = -1\n",
        "                \n",
        "        image = putText_korean(image, text, position, font_path, font_size, color)\n",
        "    \n",
        "    \n",
        "    # image = cv2.resize(image, (1440, 640))\n",
        "    cv2.imshow('frame',image)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "    \n",
        "    \n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "cv2.waitkey()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "cap_device = 0\n",
        "cap = cv2.VideoCapture(cap_device) # for Mac\n",
        "# cap = cv2.VideoCapture(0) # for Windows\n",
        "lstm_model.eval()\n",
        "status = 'None'\n",
        "status_num = -1\n",
        "step = 0\n",
        "wait_frames = 60\n",
        "cnt_frames = 0\n",
        "xyz_list_list = []\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    \n",
        "    image = cv2.resize(frame, (640, 640))\n",
        "\n",
        "    box_results = step_model.predict(image, conf = 0.6, verbose=False, show = False)\n",
        "    boxes = box_results[0].boxes.xyxy.cpu()\n",
        "    box_class = box_results[0].boxes.cls.cpu().tolist()\n",
        "\n",
        "    x1, y1, x2, y2 = 0, 0, 0, 0\n",
        "    for idx, cls in enumerate(box_class):\n",
        "        if int(cls) == step:\n",
        "            x1, y1, x2, y2 = boxes[idx]\n",
        "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
        "            cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (225, 0, 0), 2)\n",
        "    \n",
        "    if step == 0 :\n",
        "        detect_cls = 0\n",
        "\n",
        "        image, xyz_list_list, status, status_num = grab_release(image, yolo_model, hand_list , lstm_model, detect_cls, hand_cls, length, xyz_list_list, status, status_num)\n",
        "\n",
        "        if status_num == 1:\n",
        "            text = 'Put the blue bridge over the red circle'\n",
        "        else:\n",
        "            text = 'Pick up the blue bridge'\n",
        "        \n",
        "\n",
        "        if x1 != 0 and y1 != 0 and x2 != 0 and y2 != 0:\n",
        "            if cnt_frames != wait_frames:\n",
        "                text = 'Good Job!!'\n",
        "                cnt_frames += 1\n",
        "            else:\n",
        "                step += 1\n",
        "                cnt_frames = 0\n",
        "                status = 'None'\n",
        "                status_num = -1\n",
        "        cv2.putText(image, text, (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0, 225), 2)\n",
        "\n",
        "    elif step == 1:\n",
        "        detect_cls = 13\n",
        "\n",
        "        image, xyz_list_list, status, status_num = grab_release(image, yolo_model, hand_list , lstm_model, detect_cls, hand_cls, length, xyz_list_list, status, status_num)\n",
        "\n",
        "        if status_num == 1:\n",
        "            text = 'Put the yellow bridge on the left side of the blue bridge'\n",
        "        else:\n",
        "            text = 'Pick up the yellow bridge'\n",
        "\n",
        "        if x1 != 0 and y1 != 0 and x2 != 0 and y2 != 0:\n",
        "            if cnt_frames != wait_frames:\n",
        "                text = 'Good Job!!'\n",
        "                cnt_frames += 1\n",
        "            else:\n",
        "                step += 1\n",
        "                cnt_frames = 0\n",
        "                status = 'None'\n",
        "                status_num = -1\n",
        "\n",
        "        cv2.putText(image, text, (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0, 225), 2)\n",
        "    \n",
        "    elif step == 2:\n",
        "        detect_cls = 4\n",
        "\n",
        "        image, xyz_list_list, status, status_num = grab_release(image, yolo_model, hand_list , lstm_model, detect_cls, hand_cls, length, xyz_list_list, status, status_num)\n",
        "\n",
        "        if status_num == 1:\n",
        "            text = 'Put the green circle under the yellow bridge'\n",
        "        else:\n",
        "            text = 'Pick up the green circle'\n",
        "\n",
        "        if x1 != 0 and y1 != 0 and x2 != 0 and y2 != 0:\n",
        "            if cnt_frames != wait_frames:\n",
        "                text = 'Good Job!!'\n",
        "                cnt_frames += 1\n",
        "            else:\n",
        "                step += 1\n",
        "                cnt_frames = 0\n",
        "                status = 'None'\n",
        "                status_num = -1\n",
        "                \n",
        "        cv2.putText(image, text, (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0, 225), 2)\n",
        "    \n",
        "    elif step == 3:\n",
        "        detect_cls = 5\n",
        "\n",
        "        image, xyz_list_list, status, status_num = grab_release(image, yolo_model, hand_list , lstm_model, detect_cls, hand_cls, length, xyz_list_list, status, status_num)\n",
        "\n",
        "        if status_num == 1:\n",
        "            text = 'Put the green cube on the top left of the yellow bridge'\n",
        "        else:\n",
        "            text = 'Pick up the green cube'\n",
        "\n",
        "        if x1 != 0 and y1 != 0 and x2 != 0 and y2 != 0:\n",
        "            if cnt_frames != wait_frames:\n",
        "                text = 'Good Job!!'\n",
        "                cnt_frames += 1\n",
        "            else:\n",
        "                step += 1\n",
        "                cnt_frames = 0\n",
        "                status = 'None'\n",
        "                status_num = -1\n",
        "                \n",
        "        cv2.putText(image, text, (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0, 225), 2)\n",
        "    \n",
        "    elif step == 4:\n",
        "        detect_cls = 3\n",
        "\n",
        "        image, xyz_list_list, status, status_num = grab_release(image, yolo_model, hand_list , lstm_model, detect_cls, hand_cls, length, xyz_list_list, status, status_num)\n",
        "\n",
        "        if status_num == 1:\n",
        "            text = 'Put the blue quarter of circle on the right side of the green cube'\n",
        "        else:\n",
        "            text = 'Pick up the blue quarter of circle'\n",
        "\n",
        "        if x1 != 0 and y1 != 0 and x2 != 0 and y2 != 0:\n",
        "            if cnt_frames != wait_frames:\n",
        "                text = 'Good Job!!'\n",
        "                # cnt_frames += 1\n",
        "            # else:\n",
        "            #     step += 1\n",
        "            #     cnt_frames = 0\n",
        "            #     status = 'None'\n",
        "            #     status_num = -1\n",
        "                \n",
        "        cv2.putText(image, text, (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0, 225), 2)\n",
        "    \n",
        "    \n",
        "    # image = cv2.resize(image, (1440, 640))\n",
        "    cv2.imshow('frame',image)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "    \n",
        "    \n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "cv2.waitkey()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
